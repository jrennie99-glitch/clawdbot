# MoltBot Environment Variables
# Copy to .env and configure for your deployment

# =============================================================================
# REQUIRED
# =============================================================================

# Gateway authentication token (REQUIRED)
# Generate a secure random string: openssl rand -hex 32
# Both frontend and backend MUST use the same value
GATEWAY_TOKEN=

# =============================================================================
# SERVER CONFIGURATION
# =============================================================================

# Server port (Coolify sets this automatically)
PORT=3000

# Internal gateway port (do not change unless necessary)
GATEWAY_PORT=8001

# Node environment
NODE_ENV=production

# =============================================================================
# LLM CONFIGURATION (IMPORTANT - read carefully!)
# =============================================================================

# Default LLM provider: groq | openrouter | ollama | anthropic | openai | google
# Groq recommended for speed and cost. Claude is fallback only.
DEFAULT_LLM_PROVIDER=groq

# Default model for the provider
# Groq: llama-3.1-8b-instant (fast), llama-3.3-70b-versatile (smart)
# OpenRouter: meta-llama/llama-3.1-8b-instruct, deepseek/deepseek-chat
# Ollama: llama3, mistral, codellama
DEFAULT_MODEL=llama-3.1-8b-instant

# Streaming mode: true | false
# IMPORTANT: Set to false for Groq/OpenRouter/Ollama to prevent stuck requests
# Only enable streaming (true) for Claude if you need real-time token output
LLM_STREAMING=false

# Request timeout in milliseconds (default: 15000 = 15 seconds)
# Prevents stuck/hanging requests. Increase only if needed for slow models.
LLM_REQUEST_TIMEOUT_MS=15000

# Max retries on failure (default: 1)
# Set to 0 for fail-fast behavior
LLM_MAX_RETRIES=1

# =============================================================================
# LLM PROVIDER API KEYS (configure at least one)
# =============================================================================

# ----- GROQ (Recommended - fast & cheap) -----
# Get your free key at: https://console.groq.com/keys
# Supports: llama, mixtral, deepseek models
GROQ_API_KEY=

# ----- OPENROUTER (Many models, pay-per-use) -----
# Get your key at: https://openrouter.ai/keys
# Supports: llama, qwen, deepseek, mixtral, and many more
OPENROUTER_API_KEY=

# ----- OPENAI-COMPATIBLE OVERRIDE -----
# Use these to point to any OpenAI-compatible API (Groq, OpenRouter, vLLM, etc.)
# If set, OPENAI_API_KEY is used for authentication
OPENAI_BASE_URL=
OPENAI_API_KEY=

# ----- ANTHROPIC (Claude) - fallback only -----
# Get your key at: https://console.anthropic.com/
# Note: Expensive. Use as fallback, not primary.
ANTHROPIC_API_KEY=

# ----- GOOGLE AI (Gemini) -----
# Get your key at: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=

# ----- MOONSHOT/KIMI -----
# Get your key at: https://platform.moonshot.cn/
MOONSHOT_API_KEY=
KIMI_API_KEY=

# =============================================================================
# LOCAL LLM (Ollama)
# =============================================================================

# Ollama server URL (must include /v1 suffix for OpenAI-compatible API)
OLLAMA_BASE_URL=http://localhost:11434/v1

# Ollama model to use (must be pulled first: ollama pull llama3)
OLLAMA_MODEL=llama3

# =============================================================================
# ADVANCED
# =============================================================================

# Trusted proxy IPs for X-Forwarded-* headers
TRUSTED_PROXIES=127.0.0.1,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16

# Enable debug mode (NOT recommended in production)
DEBUG=false
